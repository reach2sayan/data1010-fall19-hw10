{"backend_state":"running","kernel":"julia-1.2","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":244838400},"metadata":{"jupytext":{"formats":"ipynb"},"language_info":{"file_extension":".jl","mimetype":"application/julia","name":"julia","version":"1.2.0"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"e3a9a0","input":"using Random, LinearAlgebra, Statistics\nusing Distributions, Plots, LaTeXStrings\ngr(fontfamily = \"Palatino\");","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"c00129","input":"Random.seed!(123)\n\nblue_points = [\n    rand(MvNormal([1.5; 1.5], [[0.1 0.0]; [0.0 0.1]]), 20)';\n    rand(MvNormal([0.0; 0.0], [[0.1 0.0]; [0.0 0.1]]), 20)'\n]\n\nred_points = [\n    rand(MvNormal([1.5; 0.0], [[0.1 0.0]; [0.0 0.1]]), 20)';\n    rand(MvNormal([0.0; 1.5], [[0.1 0.0]; [0.0 0.1]]), 20)'\n]\n\nplot(xlabel=L\"x_1\", ylabel=L\"x_2\", legend = false, ratio = 1, size = (300, 300))\nscatter!(blue_points[:,1], blue_points[:,2], color=:blue)\nscatter!(red_points[:,1], red_points[:,2], color=:red)","output":{"0":{"data":{"image/svg+xml":"60a759f6df9e969ab2bdee4bf7ba8ff521d84498"},"exec_count":1,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"4d0ed0","input":"x = rand(Uniform(0, 100), 1000)\ny_mean = -x .+ 0.2\ny = y_mean + rand(Normal(0, 1.5), 1000)\nscatter(x, y)","output":{"0":{"data":{"image/svg+xml":"46bb12b32938d4f8d781c1899776d5a7ee94d420"},"exec_count":2,"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"049940","input":"Consider the XOR classification problem. We have to find a decision boundary separating points labelled in blue and red, shown below:","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"5a8454","input":"## Homework 10\n\n#### *DATA 1010*","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"5e4947","input":"---\n\n## Problem 7\n\nIn this exercise, we will develop a MCMC sampler for linear regression. First, we generate some data, assuming the true regression function:\n\n$$ r(x) = -x + 0.2 $$","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"6dac1d","input":"---\n\n## Problem 8\n\nFor convenience, define the pdf for a normal distribution as\n$$f(x; \\mu, \\eta) = \\left( \\frac{\\eta}{2\\pi} \\right)^{\\frac{1}{2}} \\text{exp}\\left(  -\\frac{1}{2}\\eta(x - \\mu)^2 \\right).$$\n\nIn other words, $\\eta$ is a place holder for $\\frac{1}{\\sigma^2}$. We call $\\eta$ the **precision** of a normal distribution.\n\nSuppose we have a prior distribution for the unknown mean of a normal distribution $ N(\\mu, \\eta_0 ^{-1})$. The prior distribution for this mean $f_M(\\mu)$ is given by $N(\\mu_0, \\eta_{p}^{-1})$. In this problem we will show computationally that the posterior mean $\\mu_{\\text{post}}$ is weighted sum of the sample mean and the prior mean. In the process, we will derive the posterior variance $\\eta_{\\text{post}}$ and observe its form.\n\n1. Show that the posterior distribution is proportional to the following expression: \n\n$$ f_{M; X}(\\mu; x) \\propto \\text{exp}\\left( -\\frac 12 [n\\eta_0(\\bar{x} - \\mu)^2 + \\eta_p(\\mu - \\mu_0^2)] \\right).$$\n\n2. Re-arrange terms to format the expression in the form of a normal distribution. You will find the mean and standard deviation of the posterior distribution:\n\n$$ \\mu_{\\text{post}} = \\frac{n\\eta_0\\bar{x} + \\mu_0 \\eta_p}{n \\eta_0 + \\eta_p}$$ and\n\n$$ \\eta_{\\text{post}} = n\\eta_0 + \\eta_p$$\n\n3. Re-write the posterior mean as a weighted sum of $\\bar{x}$ and $\\mu_0$. Explain how these results agree with the observation that posterior parameters are functions of both the data and the prior distribution.\n\n*Hint:* Without proof, you may use the identity: $\\sum_{i=1}^n(x_i - \\mu)^2 = \\sum_{i=1}^n(x_i - \\bar{x})^2 + n(\\mu - \\bar{x})^2$ in your calculations.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"a59032","input":"(a) For each of the following models, determine whether the model is an appropriate choice for solving the XOR classification problem: LDA, QDA, Naive Bayes, Linear SVM, Kernelized SVM, Logistic Regression, Decision Tree. Provide a brief justification for your answer. \n\n(b) Explore different architectures on [Tensorflow Playground](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.40593&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) to find the smallest neural network that can accurately classify these points. Write down the weights and biases of each hidden layer as a matrix. Draw a diagram illustrating the architecture you selected and write an expression for the function ($x_1$ and $x_2$) that computes the output of the final layer. ","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b8470e","input":"## Problem 4\n\n(a) PCA does not take any measures to separate points from different classes. Come up with an example of a two-class dataset in $\\mathbb{R}^2$ with the property that the classes are separable in the original space, but they get all mixed up when projected to the first principal component. Please illustrate your example by generating points and making a scatter plot.\n\n(b) What about t-SNE? Use the `TSne` package to reduce your dataset from 2 dimensions to 1, and plot the results to see how it compares to PCA.","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"c78b9c","input":"Use MCMC sampling to fit a linear regression model to predict $y$ given $x$. Use $\\mathcal{N}(0, 100)$ prior for the intercept and the slope and $\\operatorname{Inv-Gamma}(0.001, 0.001)$ prior for the standard deviation. Generate 10,000 samples for each of the model parameters.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"d84cf8","input":"---\n\n## Problem 1\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"da7ef4","input":"## Problem 3\n\n(a) Consider the coordinates of $n$ points in $\\mathbb{R}^p$, organized into an $n\\times p$ matrix $A$. Suppose that `U, Σ, V = svd(A .- mean(A, dims=1))`, and explain why `V[:,1:k]'` is the matrix which maps any point in $\\mathbb{R}^p$ to its coordinates in the subspace of $\\mathbb{R}^p$ spanned by the columns of `V[:,1:k]`. (Note: this one is linear algebra review.)\n\n(b) Plot an image of the *third* principal component for the MNIST dataset. Identify a digit which you think should predominantly have a large or small dot product with this image, and make a scatter plot of which shows the dot product with the first principal component on the $x$-axis and the dot product with the third principal component on the $y$-axis. Check whether your prediction was accurate.\n\n(c) What do you think the 100th principal component might look like, compared to the first few? Display it and check your prediction.","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"edc26b","input":"--- \n\n## Problem 2\n\nIn this exercise we explore the connection between SVM and single-layer perceptron model.\n\nSuppose we have a training dataset D of size $N$: $$ D = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots (\\mathbf{x}_N, y_N)\\}, $$ where $\\mathbf{x}_i \\in \\mathbb{R}^n$, $y_i \\in \\{+1, -1\\}$. Suppose that our points are linearly separable.\n\nDefine $\\mathbf{w} \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. Define $$\\Phi(\\mathbf{x}, \\mathbf{w}, b) = \\mathbf{w} \\cdot \\mathbf{x} + b.$$ \n\n(a) Find a function $g$ and classification criteria such that $g(\\Phi(\\mathbf{x}, \\mathbf{w}, b))$ and the classification criteria give a logistic regression classifier, an SVM classifier, and a perceptron classifier.\n\n(b) Use the loss functions for perceptron and SVM classifiers to explain why a hard-margin SVM can be viewed as a perceptron with additional penalty terms given a linearly separable classification problem. \n\n(c) What advantages does the hard-margin SVM have over the single-layer perceptron when it comes to training?","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"f1eb7c","input":"---\n\n## Problem 6\n\nConsider the state space $X = \\{0, 1\\}^n$ of binary strings having length $n$. Define $p(y, x) = 1/n$ if $y$ differs from $x$ in exactly one bit, and $p(y, x) = 0$ otherwise. \n\nSuppose we desire an equilibrium distribution $\\pi$ for which $\\pi(x)$ is proportional to the number of ones that occur in vector $\\mathbf{x}$. For example, in the long run, a random walk should visit a string having five 1’s five times as often as it visits a string having only a single 1. \n\nProvide a general formula for the acceptance ratio $\\alpha(x, y)$ that would be used if we were to obtain the desired equilibrium distribution using the Metropolis-Hastings algorithm.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"f7cb1b","input":"---\n\n## Problem 5\n\nA weather station in Providence classifies each day's weather as \"good\", \"fair\", or \"poor\" according to meteorological data. The following table shows the probabilistic relationship between weather on the current day and the probability of the weather expected on next day conditioned on the type of current day.\n\ncurrent\\next | good | fair | poor\n------------| -----|-----| ----- \ngood | 0.60 | 0.30 | 0.10\nfair | 0.50 | 0.25 | 0.25\npoor | 0.20 | 0.40 | 0.40\n\n(a) Determine the probability that the weather will be \"poor\" exactly 3 days after a \"good\" weather day.\n\n(b) Over a long period of time, what percentage of days can we expect to have \"good\" weather?","pos":9,"type":"cell"}
{"id":0,"time":1573495428803,"type":"user"}
{"last_load":1573495429452,"type":"file"}